{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131cb3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903e8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result for 'SELECT * FROM employees LIMIT 5': [[1, 'Alice Johnson', 'Engineering', 95000, datetime.date(2022, 1, 15)], [2, 'Bob Smith', 'Marketing', 75000, datetime.date(2022, 3, 20)], [3, 'Carol Davis', 'Engineering', 105000, datetime.date(2021, 11, 10)], [4, 'David Brown', 'Sales', 65000, datetime.date(2022, 2, 28)], [5, 'Eve Wilson', 'Engineering', 88000, datetime.date(2022, 4, 5)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'Alice Johnson', 'Engineering', 95000, datetime.date(2022, 1, 15)],\n",
       " [2, 'Bob Smith', 'Marketing', 75000, datetime.date(2022, 3, 20)],\n",
       " [3, 'Carol Davis', 'Engineering', 105000, datetime.date(2021, 11, 10)],\n",
       " [4, 'David Brown', 'Sales', 65000, datetime.date(2022, 2, 28)],\n",
       " [5, 'Eve Wilson', 'Engineering', 88000, datetime.date(2022, 4, 5)]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def basic_trino_connection_and_query(query: str):\n",
    "     \n",
    "    conn = trino.dbapi.connect(\n",
    "        host='localhost',\n",
    "        port=8080,\n",
    "        user='admin',  \n",
    "        catalog='postgresql',   \n",
    "        schema='public'        \n",
    "    )\n",
    "    \n",
    "    # Create cursor\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Execute simple query\n",
    "    cur.execute(query)\n",
    "\n",
    "    result = cur.fetchall()\n",
    "    print(f\"Query result for '{query}':\", result)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return result\n",
    "    \n",
    "basic_trino_connection_and_query(\"SELECT * FROM employees LIMIT 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker exec -it postgres-db psql -U postgres -d testdb\n",
    "\n",
    "# for check tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Advanced Trino configuration and connection management\n",
    "\"\"\"\n",
    "import trino\n",
    "import ssl\n",
    "from typing import Optional, Dict, Any\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class TrinoConnection:\n",
    "    def __init__(\n",
    "        self,\n",
    "        host: str = 'localhost',\n",
    "        port: int = 8080,\n",
    "        user: str = 'admin',\n",
    "        catalog: Optional[str] = None,\n",
    "        schema: Optional[str] = None,\n",
    "        auth: Optional[Any] = None,\n",
    "        use_ssl: bool = False,\n",
    "        ssl_verify: bool = True,\n",
    "        properties: Optional[Dict[str, str]] = None\n",
    "    ):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.user = user\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.auth = auth\n",
    "        self.use_ssl = use_ssl\n",
    "        self.ssl_verify = ssl_verify\n",
    "        self.properties = properties or {}\n",
    "        \n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Trino\"\"\"\n",
    "        connection_params = {\n",
    "            'host': self.host,\n",
    "            'port': self.port,\n",
    "            'user': self.user,\n",
    "            'catalog': self.catalog,\n",
    "            'schema': self.schema,\n",
    "            'source': 'python-client',\n",
    "            'isolation_level': trino.constants.ISOLATION_LEVEL.READ_COMMITTED\n",
    "        }\n",
    "        \n",
    "        # Add authentication if provided\n",
    "        if self.auth:\n",
    "            connection_params['auth'] = self.auth\n",
    "        \n",
    "        # Add SSL configuration\n",
    "        if self.use_ssl:\n",
    "            if not self.ssl_verify:\n",
    "                connection_params['http_scheme'] = 'https'\n",
    "                connection_params['verify'] = False\n",
    "            else:\n",
    "                connection_params['http_scheme'] = 'https'\n",
    "        \n",
    "        # Add session properties\n",
    "        if self.properties:\n",
    "            connection_params['session_properties'] = self.properties\n",
    "        \n",
    "        self.connection = trino.dbapi.connect(**connection_params)\n",
    "        self.cursor = self.connection.cursor()\n",
    "        \n",
    "        print(f\"Connected to Trino at {self.host}:{self.port}\")\n",
    "        return self.connection\n",
    "    \n",
    "    def execute_query(self, query: str, parameters: Optional[tuple] = None):\n",
    "        \"\"\"Execute a query and return results\"\"\"\n",
    "        if not self.cursor:\n",
    "            raise RuntimeError(\"No active connection. Call connect() first.\")\n",
    "        \n",
    "        try:\n",
    "            if parameters:\n",
    "                self.cursor.execute(query, parameters)\n",
    "            else:\n",
    "                self.cursor.execute(query)\n",
    "            \n",
    "            return self.cursor.fetchall()\n",
    "        except Exception as e:\n",
    "            print(f\"Query execution error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_query_info(self):\n",
    "        \"\"\"Get information about the last executed query\"\"\"\n",
    "        if self.cursor:\n",
    "            return self.cursor.stats\n",
    "        return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the connection\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            print(\"Connection closed\")\n",
    "\n",
    "# Example usage with different configurations\n",
    "def example_connections():\n",
    "    # Basic connection\n",
    "    basic_conn = TrinoConnection(\n",
    "        catalog='memory',\n",
    "        schema='default'\n",
    "    )\n",
    "    basic_conn.connect()\n",
    "    \n",
    "    # Test memory catalog\n",
    "    result = basic_conn.execute_query(\"SELECT 'Hello Trino!' as message\")\n",
    "    print(\"Memory catalog test:\", result)\n",
    "    basic_conn.close()\n",
    "    \n",
    "    # PostgreSQL connection\n",
    "    pg_conn = TrinoConnection(\n",
    "        catalog='postgresql',\n",
    "        schema='public',\n",
    "        properties={\n",
    "            'query_max_execution_time': '1h',\n",
    "            'query_max_memory': '1GB'\n",
    "        }\n",
    "    )\n",
    "    pg_conn.connect()\n",
    "    \n",
    "    # Query PostgreSQL through Trino\n",
    "    employees = pg_conn.execute_query(\"SELECT * FROM employees LIMIT 5\")\n",
    "    print(\"\\nEmployees from PostgreSQL:\")\n",
    "    for emp in employees:\n",
    "        print(f\"  {emp}\")\n",
    "    \n",
    "    pg_conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Trino integration with Pandas for data analysis\n",
    "\"\"\"\n",
    "import trino\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "class TrinoPandas:\n",
    "    def __init__(self, host='localhost', port=8080, user='admin'):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.user = user\n",
    "        \n",
    "        # Create SQLAlchemy engine for pandas integration\n",
    "        self.engine = create_engine(\n",
    "            URL.create(\n",
    "                drivername=\"trino\",\n",
    "                host=host,\n",
    "                port=port,\n",
    "                username=user,\n",
    "                database=\"postgresql\",  # Default catalog\n",
    "                query={\"schema\": \"public\"}\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def query_to_dataframe(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Execute query and return results as pandas DataFrame\"\"\"\n",
    "        try:\n",
    "            df = pd.read_sql_query(query, self.engine)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def dataframe_to_trino(self, df: pd.DataFrame, table_name: str, \n",
    "                          catalog: str = 'memory', schema: str = 'default'):\n",
    "        \"\"\"Upload DataFrame to Trino (memory catalog for testing)\"\"\"\n",
    "        # For memory catalog, we need to create table with values\n",
    "        # This is a simplified example - in production, use proper ETL\n",
    "        \n",
    "        conn = trino.dbapi.connect(\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            user=self.user,\n",
    "            catalog=catalog,\n",
    "            schema=schema\n",
    "        )\n",
    "        \n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Generate CREATE TABLE AS SELECT statement\n",
    "        columns = []\n",
    "        values_list = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # Determine Trino data type based on pandas dtype\n",
    "            if df[col].dtype == 'int64':\n",
    "                trino_type = 'BIGINT'\n",
    "            elif df[col].dtype == 'float64':\n",
    "                trino_type = 'DOUBLE'\n",
    "            elif df[col].dtype == 'bool':\n",
    "                trino_type = 'BOOLEAN'\n",
    "            else:\n",
    "                trino_type = 'VARCHAR'\n",
    "            \n",
    "            columns.append(f\"{col} {trino_type}\")\n",
    "        \n",
    "        # Create table\n",
    "        create_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            {', '.join(columns)}\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        cur.execute(create_query)\n",
    "        \n",
    "        # Insert data (simplified - in practice, use bulk insert methods)\n",
    "        for _, row in df.head(100).iterrows():  # Limit for demo\n",
    "            values = []\n",
    "            for val in row:\n",
    "                if pd.isna(val):\n",
    "                    values.append('NULL')\n",
    "                elif isinstance(val, str):\n",
    "                    values.append(f\"'{val.replace('\\'', '\\'\\'')}'\")\n",
    "                else:\n",
    "                    values.append(str(val))\n",
    "            \n",
    "            insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} VALUES ({', '.join(values)})\n",
    "            \"\"\"\n",
    "            cur.execute(insert_query)\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"DataFrame uploaded to {catalog}.{schema}.{table_name}\")\n",
    "\n",
    "def data_analysis_examples():\n",
    "    \"\"\"Examples of data analysis with Trino and Pandas\"\"\"\n",
    "    tp = TrinoPandas()\n",
    "    \n",
    "    # Employee analysis\n",
    "    print(\"=== Employee Analysis ===\")\n",
    "    employees_df = tp.query_to_dataframe(\"\"\"\n",
    "        SELECT \n",
    "            name,\n",
    "            department,\n",
    "            salary,\n",
    "            hire_date,\n",
    "            EXTRACT(YEAR FROM hire_date) as hire_year\n",
    "        FROM employees\n",
    "        ORDER BY salary DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Employee DataFrame shape:\", employees_df.shape)\n",
    "    print(\"\\nFirst 5 employees:\")\n",
    "    print(employees_df.head())\n",
    "    \n",
    "    print(\"\\nSalary statistics by department:\")\n",
    "    dept_stats = employees_df.groupby('department')['salary'].agg(['mean', 'min', 'max', 'count'])\n",
    "    print(dept_stats)\n",
    "    \n",
    "    # Sales analysis with joins\n",
    "    print(\"\\n=== Sales Analysis ===\")\n",
    "    sales_analysis = tp.query_to_dataframe(\"\"\"\n",
    "        SELECT \n",
    "            s.product_name,\n",
    "            s.category,\n",
    "            s.amount,\n",
    "            s.sale_date,\n",
    "            e.name as employee_name,\n",
    "            e.department as employee_department\n",
    "        FROM sales s\n",
    "        JOIN employees e ON s.employee_id = e.id\n",
    "        ORDER BY s.amount DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Sales with employee info:\")\n",
    "    print(sales_analysis.head())\n",
    "    \n",
    "    # Aggregated sales by category\n",
    "    category_sales = tp.query_to_dataframe(\"\"\"\n",
    "        SELECT \n",
    "            s.category,\n",
    "            COUNT(*) as number_of_sales,\n",
    "            SUM(s.amount) as total_sales,\n",
    "            AVG(s.amount) as average_sale,\n",
    "            MIN(s.amount) as min_sale,\n",
    "            MAX(s.amount) as max_sale\n",
    "        FROM sales s\n",
    "        GROUP BY s.category\n",
    "        ORDER BY total_sales DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nSales by category:\")\n",
    "    print(category_sales)\n",
    "    \n",
    "    # Complex analytical query\n",
    "    print(\"\\n=== Advanced Analytics ===\")\n",
    "    advanced_analysis = tp.query_to_dataframe(\"\"\"\n",
    "        WITH monthly_sales AS (\n",
    "            SELECT \n",
    "                EXTRACT(MONTH FROM sale_date) as month,\n",
    "                EXTRACT(YEAR FROM sale_date) as year,\n",
    "                category,\n",
    "                SUM(amount) as monthly_total\n",
    "            FROM sales\n",
    "            GROUP BY EXTRACT(YEAR FROM sale_date), EXTRACT(MONTH FROM sale_date), category\n",
    "        ),\n",
    "        employee_performance AS (\n",
    "            SELECT \n",
    "                e.name,\n",
    "                e.department,\n",
    "                COUNT(s.id) as sales_count,\n",
    "                SUM(s.amount) as total_sales,\n",
    "                AVG(s.amount) as avg_sale\n",
    "            FROM employees e\n",
    "            LEFT JOIN sales s ON e.id = s.employee_id\n",
    "            GROUP BY e.name, e.department\n",
    "        )\n",
    "        SELECT * FROM employee_performance\n",
    "        WHERE sales_count > 0\n",
    "        ORDER BY total_sales DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Employee performance:\")\n",
    "    print(advanced_analysis)\n",
    "    \n",
    "    # Create some visualizations\n",
    "    create_visualizations(employees_df, sales_analysis, category_sales)\n",
    "\n",
    "def create_visualizations(employees_df, sales_analysis, category_sales):\n",
    "    \"\"\"Create visualizations using Plotly\"\"\"\n",
    "    \n",
    "    # Salary distribution by department\n",
    "    fig1 = px.box(employees_df, x='department', y='salary', \n",
    "                  title='Salary Distribution by Department')\n",
    "    fig1.show()\n",
    "    \n",
    "    # Sales by category\n",
    "    fig2 = px.bar(category_sales, x='category', y='total_sales',\n",
    "                  title='Total Sales by Category')\n",
    "    fig2.show()\n",
    "    \n",
    "    # Sales timeline\n",
    "    sales_timeline = sales_analysis.groupby('sale_date')['amount'].sum().reset_index()\n",
    "    fig3 = px.line(sales_timeline, x='sale_date', y='amount',\n",
    "                   title='Sales Timeline')\n",
    "    fig3.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_analysis_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2312b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Federated queries across multiple data sources\n",
    "\"\"\"\n",
    "import trino\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class TrinoFederatedQueries:\n",
    "    def __init__(self, host='localhost', port=8080, user='admin'):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.user = user\n",
    "    \n",
    "    def get_connection(self, catalog=None, schema=None):\n",
    "        \"\"\"Get Trino connection for specific catalog/schema\"\"\"\n",
    "        return trino.dbapi.connect(\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            user=self.user,\n",
    "            catalog=catalog,\n",
    "            schema=schema\n",
    "        )\n",
    "    \n",
    "    def cross_catalog_query(self):\n",
    "        \"\"\"Example of querying across multiple catalogs\"\"\"\n",
    "        conn = self.get_connection()\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Query combining PostgreSQL and memory catalogs\n",
    "        federated_query = \"\"\"\n",
    "        SELECT \n",
    "            pg_emp.name,\n",
    "            pg_emp.department,\n",
    "            pg_emp.salary,\n",
    "            mem_data.bonus_percentage\n",
    "        FROM postgresql.public.employees pg_emp\n",
    "        CROSS JOIN (\n",
    "            VALUES \n",
    "                ('Engineering', 0.15),\n",
    "                ('Marketing', 0.12),\n",
    "                ('Sales', 0.10)\n",
    "        ) AS mem_data(department, bonus_percentage)\n",
    "        WHERE pg_emp.department = mem_data.department\n",
    "        ORDER BY pg_emp.salary DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        cur.execute(federated_query)\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        print(\"Cross-catalog federated query results:\")\n",
    "        for row in results:\n",
    "            name, dept, salary, bonus = row\n",
    "            bonus_amount = salary * bonus\n",
    "            print(f\"{name} ({dept}): ${salary:,} + ${bonus_amount:,.2f} bonus\")\n",
    "        \n",
    "        conn.close()\n",
    "    \n",
    "    def window_functions_example(self):\n",
    "        \"\"\"Advanced analytics with window functions\"\"\"\n",
    "        conn = self.get_connection(catalog='postgresql', schema='public')\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        window_query = \"\"\"\n",
    "        SELECT \n",
    "            name,\n",
    "            department,\n",
    "            salary,\n",
    "            RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dept_salary_rank,\n",
    "            DENSE_RANK() OVER (ORDER BY salary DESC) as overall_salary_rank,\n",
    "            LAG(salary) OVER (PARTITION BY department ORDER BY salary) as prev_salary_in_dept,\n",
    "            AVG(salary) OVER (PARTITION BY department) as dept_avg_salary,\n",
    "            salary - AVG(salary) OVER (PARTITION BY department) as salary_vs_dept_avg\n",
    "        FROM employees\n",
    "        ORDER BY department, salary DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        cur.execute(window_query)\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        print(\"\\nWindow functions analysis:\")\n",
    "        for row in results:\n",
    "            print(f\"{row[0]} - Dept Rank: {row[3]}, Overall Rank: {row[4]}, \"\n",
    "                  f\"vs Dept Avg: ${row[7]:.2f}\")\n",
    "        \n",
    "        conn.close()\n",
    "    \n",
    "    def array_and_json_operations(self):\n",
    "        \"\"\"Working with complex data types\"\"\"\n",
    "        conn = self.get_connection(catalog='memory', schema='default')\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Create table with complex types\n",
    "        complex_query = \"\"\"\n",
    "        WITH sample_data AS (\n",
    "            SELECT * FROM VALUES\n",
    "            (1, 'John', ARRAY['python', 'sql', 'java'], \n",
    "             JSON '{\"experience\": 5, \"certifications\": [\"aws\", \"gcp\"]}'),\n",
    "            (2, 'Jane', ARRAY['python', 'r', 'scala'], \n",
    "             JSON '{\"experience\": 7, \"certifications\": [\"azure\", \"databricks\"]}'),\n",
    "            (3, 'Bob', ARRAY['java', 'javascript', 'go'], \n",
    "             JSON '{\"experience\": 3, \"certifications\": [\"kubernetes\"]}')\n",
    "        ) AS t(id, name, skills, profile)\n",
    "        \n",
    "        SELECT \n",
    "            name,\n",
    "            CARDINALITY(skills) as num_skills,\n",
    "            ARRAY_JOIN(skills, ', ') as skills_list,\n",
    "            JSON_EXTRACT_SCALAR(profile, '$.experience') as experience_years,\n",
    "            JSON_EXTRACT(profile, '$.certifications') as certifications\n",
    "        FROM sample_data\n",
    "        WHERE CONTAINS(skills, 'python')\n",
    "        \"\"\"\n",
    "        \n",
    "        cur.execute(complex_query)\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        print(\"\\nComplex data types analysis:\")\n",
    "        for row in results:\n",
    "            print(f\"{row[0]}: {row[2]} ({row[1]} skills), \"\n",
    "                  f\"{row[3]} years experience\")\n",
    "        \n",
    "        conn.close()\n",
    "\n",
    "def performance_monitoring():\n",
    "    \"\"\"Monitor query performance and statistics\"\"\"\n",
    "    conn = trino.dbapi.connect(\n",
    "        host='localhost',\n",
    "        port=8080,\n",
    "        user='admin',\n",
    "        catalog='postgresql',\n",
    "        schema='public'\n",
    "    )\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Execute a sample query\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as employee_count,\n",
    "        AVG(salary) as avg_salary,\n",
    "        STDDEV(salary) as salary_stddev\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    cur.execute(query)\n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    # Get query statistics\n",
    "    stats = cur.stats\n",
    "    if stats:\n",
    "        print(\"Query Performance Statistics:\")\n",
    "        print(f\"  Query ID: {stats.get('queryId', 'N/A')}\")\n",
    "        print(f\"  Elapsed Time: {stats.get('elapsedTimeMillis', 0)} ms\")\n",
    "        print(f\"  CPU Time: {stats.get('cpuTimeMillis', 0)} ms\")\n",
    "        print(f\"  Processed Rows: {stats.get('processedRows', 0)}\")\n",
    "        print(f\"  Processed Bytes: {stats.get('processedBytes', 0)}\")\n",
    "        print(f\"  Peak Memory: {stats.get('peakMemoryBytes', 0)} bytes\")\n",
    "    \n",
    "    print(\"\\nQuery Results:\")\n",
    "    for row in results:\n",
    "        dept, count, avg_sal, stddev = row\n",
    "        print(f\"  {dept}: {count} employees, \"\n",
    "              f\"avg ${avg_sal:.2f}, stddev ${stddev:.2f}\")\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tfq = TrinoFederatedQueries()\n",
    "    \n",
    "    print(\"=== Cross-Catalog Queries ===\")\n",
    "    tfq.cross_catalog_query()\n",
    "    \n",
    "    print(\"\\n=== Window Functions ===\")\n",
    "    tfq.window_functions_example()\n",
    "    \n",
    "    print(\"\\n=== Complex Data Types ===\")\n",
    "    tfq.array_and_json_operations()\n",
    "    \n",
    "    print(\"\\n=== Performance Monitoring ===\")\n",
    "    performance_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ETL pipeline using Python and Trino\n",
    "\"\"\"\n",
    "import trino\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import csv\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "class TrinoETL:\n",
    "    def __init__(self, host='localhost', port=8080, user='admin'):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.user = user\n",
    "    \n",
    "    def get_connection(self, catalog='memory', schema='default'):\n",
    "        return trino.dbapi.connect(\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            user=self.user,\n",
    "            catalog=catalog,\n",
    "            schema=schema\n",
    "        )\n",
    "    \n",
    "    def create_sample_data(self):\n",
    "        \"\"\"Generate sample data files for ETL demonstration\"\"\"\n",
    "        \n",
    "        # Generate customer data\n",
    "        customers = []\n",
    "        for i in range(1000):\n",
    "            customer = {\n",
    "                'customer_id': i + 1,\n",
    "                'first_name': f'Customer{i+1}',\n",
    "                'last_name': f'LastName{i+1}',\n",
    "                'email': f'customer{i+1}@example.com',\n",
    "                'registration_date': (datetime.now() - timedelta(days=np.random.randint(1, 365))).strftime('%Y-%m-%d'),\n",
    "                'country': np.random.choice(['US', 'UK', 'CA', 'DE', 'FR'], p=[0.4, 0.2, 0.15, 0.15, 0.1]),\n",
    "                'age': np.random.randint(18, 80)\n",
    "            }\n",
    "            customers.append(customer)\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open('customers.json', 'w') as f:\n",
    "            for customer in customers:\n",
    "                f.write(json.dumps(customer) + '\\n')\n",
    "        \n",
    "        # Generate orders data as CSV\n",
    "        orders = []\n",
    "        for i in range(5000):\n",
    "            order = {\n",
    "                'order_id': i + 1,\n",
    "                'customer_id': np.random.randint(1, 1001),\n",
    "                'product_name': np.random.choice([\n",
    "                    'Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones',\n",
    "                    'Tablet', 'Phone', 'Speaker', 'Camera', 'Printer'\n",
    "                ]),\n",
    "                'quantity': np.random.randint(1, 5),\n",
    "                'unit_price': round(np.random.uniform(10, 1000), 2),\n",
    "                'order_date': (datetime.now() - timedelta(days=np.random.randint(1, 90))).strftime('%Y-%m-%d'),\n",
    "                'status': np.random.choice(['completed', 'pending', 'cancelled'], p=[0.8, 0.15, 0.05])\n",
    "            }\n",
    "            order['total_amount'] = round(order['quantity'] * order['unit_price'], 2)\n",
    "            orders.append(order)\n",
    "        \n",
    "        # Save as CSV\n",
    "        df_orders = pd.DataFrame(orders)\n",
    "        df_orders.to_csv('orders.csv', index=False)\n",
    "        \n",
    "        print(\"Sample data files created: customers.json, orders.csv\")\n",
    "    \n",
    "    def load_json_data(self):\n",
    "        \"\"\"Load JSON data into Trino memory catalog\"\"\"\n",
    "        conn = self.get_connection()\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Create customers table\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS customers (\n",
    "                customer_id BIGINT,\n",
    "                first_name VARCHAR,\n",
    "                last_name VARCHAR,\n",
    "                email VARCHAR,\n",
    "                registration_date DATE,\n",
    "                country VARCHAR,\n",
    "                age INTEGER\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Load JSON data\n",
    "        with open"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trino-start",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
